'An HTTP handler for urllib2 that supports HTTP 1.1 and keepalive.\n\n>>> import urllib2\n>>> from keepalive import HTTPHandler\n>>> keepalive_handler = HTTPHandler()\n>>> opener = urllib2.build_opener(keepalive_handler)\n>>> urllib2.install_opener(opener)\n>>>\n>>> fo = urllib2.urlopen(\'http://www.python.org\')\n\nIf a connection to a given host is requested, and all of the existing\nconnections are still in use, another connection will be opened.  If\nthe handler tries to use an existing connection but it fails in some\nway, it will be closed and removed from the pool.\n\nTo remove the handler, simply re-run build_opener with no arguments, and\ninstall that opener.\n\nYou can explicitly close connections by using the close_connection()\nmethod of the returned file-like object (described below) or you can\nuse the handler methods:\n\n  close_connection(host)\n  close_all()\n  open_connections()\n\nNOTE: using the close_connection and close_all methods of the handler\nshould be done with care when using multiple threads.\n  * there is nothing that prevents another thread from creating new\n    connections immediately after connections are closed\n  * no checks are done to prevent in-use connections from being closed\n\n>>> keepalive_handler.close_all()\n\nEXTRA ATTRIBUTES AND METHODS\n\n  Upon a status of 200, the object returned has a few additional\n  attributes and methods, which should not be used if you want to\n  remain consistent with the normal urllib2-returned objects:\n\n    close_connection()  -  close the connection to the host\n    readlines()         -  you know, readlines()\n    status              -  the return status (ie 404)\n    reason              -  english translation of status (ie \'File not found\')\n\n  If you want the best of both worlds, use this inside an\n  AttributeError-catching try:\n\n  >>> try: status = fo.status\n  >>> except AttributeError: status = None\n\n  Unfortunately, these are ONLY there if status == 200, so it\'s not\n  easy to distinguish between non-200 responses.  The reason is that\n  urllib2 tries to do clever things with error codes 301, 302, 401,\n  and 407, and it wraps the object upon return.\n\n  For python versions earlier than 2.4, you can avoid this fancy error\n  handling by setting the module-level global HANDLE_ERRORS to zero.\n  You see, prior to 2.4, it\'s the HTTP Handler\'s job to determine what\n  to handle specially, and what to just pass up.  HANDLE_ERRORS == 0\n  means "pass everything up".  In python 2.4, however, this job no\n  longer belongs to the HTTP Handler and is now done by a NEW handler,\n  HTTPErrorProcessor.  Here\'s the bottom line:\n\n    python version < 2.4\n        HANDLE_ERRORS == 1  (default) pass up 200, treat the rest as\n                            errors\n        HANDLE_ERRORS == 0  pass everything up, error processing is\n                            left to the calling code\n    python version >= 2.4\n        HANDLE_ERRORS == 1  pass up 200, treat the rest as errors\n        HANDLE_ERRORS == 0  (default) pass everything up, let the\n                            other handlers (specifically,\n                            HTTPErrorProcessor) decide what to do\n\n  In practice, setting the variable either way makes little difference\n  in python 2.4, so for the most consistent behavior across versions,\n  you probably just want to use the defaults, which will give you\n  exceptions on errors.\n\n'
import errno
import httplib
import socket
import thread
import urllib2
DEBUG = None
import sys
if (sys.version_info < (2, 4)):
    HANDLE_ERRORS = 1
else:
    HANDLE_ERRORS = 0
if (__name__ == '__main__'):
    import time
    import sys
    try:
        N = int(sys.argv[1])
        url = sys.argv[2]
    except:
        print ('%s <integer> <url>' % sys.argv[0])
    else:
        test(url, N)
